{
  "story": {
    "by": "ComputerGuru",
    "descendants": 149,
    "id": 46890335,
    "kids": [
      46906794,
      46907841,
      46907147,
      46906897,
      46906904,
      46907459,
      46907982,
      46906725,
      46909057,
      46907157,
      46908271,
      46907669,
      46907329,
      46907602,
      46909736,
      46906520,
      46906642,
      46910139,
      46906653,
      46908397,
      46909065,
      46906910,
      46908529,
      46907053,
      46909063,
      46909125,
      46907346,
      46906832,
      46910081,
      46910882
    ],
    "score": 412,
    "time": 1770232747,
    "title": "Recreating Epstein PDFs from raw encoded attachments",
    "type": "story",
    "url": "https://neosmart.net/blog/recreating-epstein-pdfs-from-raw-encoded-attachments/"
  },
  "article_text": "Recreating uncensored Epstein PDFs from raw encoded attachments | The NeoSmart FilesThe NeoSmart Files There have been a lot of complaints about both the competency and the logic behind the latest Epstein archive release by the DoJ: from censoring the names of co-conspirators to censoring pictures of random women in a way that makes individuals look guiltier than they really are , forgetting to redact credentials that made it possible for all of Reddit to log into Epstein\u2019s account and trample over all the evidence, and the complete ineptitude that resulted in most of the latest batch being corrupted thanks to incorrectly converted Quoted-Printable encoding artifacts , it\u2019s safe to say that Pam Bondi\u2019s DoJ did not put its best and brightest on this (admittedly gargantuan) undertaking. But the most damning evidence has all been thoroughly redacted\u2026 hasn\u2019t it? Well, maybe not. I was thinking of writing an article on the mangled quoted-printable encoding the day this latest dump came out in response to all the misinformed musings and conjectures that were littering social media (and my dilly-dallying cost me, as someone beat me to the punch ), and spent some time searching through the latest archives looking\u00a0 for some SMTP headers that I could use in the article when I came across a curious artifact: not only were the emails badly transcoded into plain text, but also some binary attachments were actually included in the dumps in their over-the-wire Content-Transfer-Encoding: base64 format, and the unlucky intern that was assigned to the documents in question didn\u2019t realize the significance of what they were looking at and didn\u2019t see the point in censoring seemingly meaningless page after page of hex content! Just take a look at EFTA00400459 , an email from correspondence between (presumably) one of Epstein\u2019s assistants and Epstein lackey/co-conspirator Boris Nikolic and his friend, Sam Jaradeh, inviting them to a \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 benefit: Those hex characters go on for 76 pages, and represent the file DBC12 One Page Invite with Reply.pdf encoded as base64 so that it can be included in the email without breaking the SMTP protocol. And converting it back to the original PDF is, theoretically, as easy as copy-and-pasting those 76 pages into a text editor, stripping the leading > bytes, and piping all that into base64 -d > output.pdf \u2026 or it would be, if we had the original (badly converted) email and not a partially redacted scan of a printout of said email with some shoddy OCR applied. If you tried to actually copy that text as digitized by the DoJ from the PDF into a text editor, here\u2019s what you\u2019d see: You can ignore the EFTA00400459 on the second line; that (or some variant thereof) will be interspersed into the base64 text since it\u2019s stamped at the bottom of every page to identify the piece of evidence it came from. But what else do you notice? Here\u2019s a hint: this is what proper base64 looks like: Notice how in this sample everything lines up perfectly (when using a monospaced font) at the right margin? And how that\u2019s not the case when we copied-and-pasted from the OCR\u2019d PDF? That\u2019s because it wasn\u2019t a great OCR job: extra characters have been hallucinated into the output, some of them not even legal base64 characters such as the , and [ , while other characters have been omitted altogether, giving us content we can\u2019t use: 1 > pbpaste \\\r\n     | string match -rv 'EFTA' \\\r\n     | string trim -c \" >\" \\\r\n     | string join \"\" \\\r\n     | base64 -d >/dev/null base64: invalid input I tried the easiest alternative I had at hand: I loaded up the PDF in Adobe Acrobat Pro and re-ran an OCR process on the document, but came up with even worse results, with spaces injected in the middle of the base64 content (easily fixable) in addition to other characters being completely misread and butchered \u2013 it really didn\u2019t like the cramped monospace text at all. So I thought to do it manually with tesseract , which, while very far from state-of-the-art, can still be useful because it lets you do things like limit its output to a certain subset of characters, constraining the field of valid results and hopefully coercing it into producing better results. Only one problem: tesseract can\u2019t read PDF input (or not by default, anyway). No problem, I\u2019ll just use imagemagick / ghostscript to convert the PDF into individual PNG images (to avoid further generational loss) and provide those to tesseract , right? But that didn\u2019t quite work out, they seem (?) to try to load and perform the conversion of all 76 separate pages/png files all at once, and then naturally crash on too-large inputs (but only after taking forever and generating the 76 (invalid) output files that you\u2019re forced to subsequently clean up, of course): > convert -density 300 EFTA00400459.pdf \\\r\n        -background white -alpha remove \\\r\n        -alpha off out.png convert-im6.q16: cache resources exhausted `/tmp/magick-QqXVSOZutVsiRcs7pLwwG2FYQnTsoAmX47' @ error/cache.c/OpenPixelCache/4119. convert-im6.q16: cache resources exhausted `out.png' @ error/cache.c/OpenPixelCache/4119. convert-im6.q16: No IDATs written into file `out-0.png' @ error/png.c/MagickPNGErrorHandler/1643. So we turn to pdftoppm from the poppler-utils package instead, which does indeed handle each page of the source PDF separately and turned out to be up to the task, though incredibly slow: > pdftoppm -png -r 300 EFTA00400459.pdf out.png After waiting the requisite amount of time (and then some), I had files out-01.png through out-76.png , and was ready to try them with tesseract : for n in (printf \"%02d\\n\" (seq 1 76))\r\n    tesseract out-$n.png output-$n \\\r\n        --psm 6 \\\r\n        -c tessedit_char_whitelist='>'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/= \\\r\n        -c load_system_dawg=0 \\\r\n        -c load_freq_dawg=0\r\nend The above fish-shell command instructs tesseract (1) to assume the input is a single block of text (the --psm 6 argument) and limit itself to decoding only legal base64 characters (and the leading > so we can properly strip it out thereafter). My original attempt included a literal space in the valid char whitelist, but that gave me worse results: the very badly kerned base64 has significant apparent spacing between some adjacent characters (more on this later) and that caused tesseract to both incorrectly inject spaces (bad but fixable) and also possibly affect how it handled the character after the space (worse). Unfortunately, while tesseract gave me slightly better output than either the original OCR\u2019d DoJ text or the (terrible) Adobe Acrobat Pro OCR results, it too suffered from poor recognition and gave me very inconsistent line lengths\u2026 but it also suffered from something that I didn\u2019t really think a heuristic-based, algorithm-driven tool like tesseract would succumb to, as it was more reminiscent of how first-generation LLMs would behave: in a few places, it would only read the first dozen or so characters on a line then leave the rest of the line blank, then pick up (correctly enough) at the start of the next line. Before I saw how generally useless the OCR results were and gave up on tesseract, I figured I\u2019d just manually type out the rest of the line (the aborted lines were easy enough to find, thanks to the monospaced output), and that was when I ran into the real issue that took this from an interesting challenge to being almost mission impossible. I mentioned earlier the bad kerning, which tricked the OCR tools into injecting spaces where there were supposed to be none, but that was far from being the worst issue plaguing the PDF content. The real problem is that the text is rendered in possibly the worst typeface for the job at hand: Courier New. If you\u2019re a font enthusiast, I certainly don\u2019t need to say any more \u2013 you\u2019re probably already shaking with a mix of PTSD and rage. But for the benefit of everyone else, let\u2019s just say that Courier New is\u2026 not a great font. It was a digitization of the venerable (though certainly primitive) Courier fontface, commissioned by IBM in the 1950s. Courier was used (with some tweaks) for IBM typewriters, including the IBM Selectric , and in the 1990s it was \u201cdigitized directly from the golf ball of the\u00a0IBM Selectric\u201d by Monotype, and shipped with Windows 3.1, where it remained the default monospace font on Windows until Consolas shipped with Windows Vista . Among the many issues with Courier New is that it was digitized from the Selectric golf ball \u201cwithout accounting for the visual weight normally added by the\u00a0typewriter\u2019s ink ribbon\u201d, which gives its characteristic \u201cthin\u201d look. Microsoft ClearType, which was only enabled by default with Windows Vista, addressed this major shortcoming to some extent, but Courier New has always struggled with general readability\u2026 and more importantly, with its poor distinction between characters. You can clearly see how downright anemic Courier New is when compared to the original Courier. While not as bad as some typewriter-era typefaces that actually reused the same symbol for 1 (one) and l (ell), Courier New came pretty close. Here is a comparison between the two fonts when rendering these two characters, only considerably enlarged: Comparing Courier and Courier New when it comes to differentiating between 1 (one) and l (ell). The combination of the two faults (the anemic weights and the even less distinction between 1 and l as compared to Courier) makes Courier New a terrible choice as a programming font. But as a font used for base64 output you want to OCR? You really couldn\u2019t pick a worse option! To add fuel to the fire, you\u2019re looking at SVG outlines of the fonts, meticulously converted and preserving all the fine details. But in the Epstein PDFs released by the DoJ, we only have low-quality JPEG scans at a fairly small point size. Here\u2019s an actual (losslessly encoded) screenshot of the DoJ text at 100% \u2013 I challenge you to tell me which is a 1 and which is an l in the excerpt below: It\u2019s not that there isn\u2019t any difference between the two, because there is. And sometimes you get a clear gut feeling which is which \u2013 I was midway through manually typing out one line of base64 text when I got stuck on identifying a one vs ell\u2026 only to realize that, at the same time, I had confidently transcribed one of them earlier that same line without even pausing to think about which it was. Here\u2019s a zoomed-in view of the scanned PDF: you can clearly see all the JPEG DCT artifacts, the color fringing, and the smearing of character shapes, all of which make it hard to properly identify the characters. But at the same time, at least in this particular sample, you can see which of the highlighted characters have a straight serif leading out the top-left (the middle, presumably an ell) and which of those have the slightest of strokes/feet extending from them (the first and last, presumably ones). But whether that\u2019s because that\u2019s how the original glyph appeared or it\u2019s because of how the image was compressed, it\u2019s tough to say: But that\u2019s getting ahead of myself: at this point, none of the OCR tools had actually given me usable results, even ignoring the very important question of l vs 1 . After having been let down by one open source offering (tesseract) and two commercial ones (Adobe Acrobat Pro and, presumably, whatever the DoJ used), I made the very questionable choice of writing a script to use yet another commercial offering, this time Amazon/AWS Textract , to process the PDF. Unfortunately, using it directly via the first-party tooling was (somewhat) of a no-go as it only supports smaller/shorter inputs for direct use; longer PDFs like this one need to be uploaded to S3 and then use the async workflow to start the recognition and poll for completion. Amazon Textract did possibly the best out of all the tools I tried, but its output still had obvious line length discrepancies \u2013 albeit only one to two characters or so off on average. I decided to try again, this time blowing up the input 2x (using nearest neighbor sampling to preserve sharp edges) as a workaround for Textract not having a tunable I could set to configure the DPI the document is processed at, though I worried all inputs could possibly be prescaled to a fixed size prior to processing once more: 2 > for n in (printf \"%02d\\n\" (seq 01 76))\r\n      convert EFTA00400459-$n.png -scale 200% \\\r\n              EFTA00400459-$n\"_2x\".png; or break\r\n  end\r\n> parallel -j 16 ./textract.sh {} ::: EFTA00400459-*_2x.png These results were notably better, and I\u2019ve included them in an archive, but some of the pages scanned better than others. Textract doesn\u2019t seem to be 100% deterministic from my brief experience with it, and their features page does make vague or unclear mentions to \u201cML\u201d, though it\u2019s not obvious when and where it kicks in or what it exactly refers to, but that could explain why a couple of the pages (like EFTA00400459-62_2x.txt ) are considerably worse than others, even while the source images don\u2019t show a good reason for that divergence. With the Textract 2x output cleaned up and piped into base64 -i (which ignores garbage data, generating invalid results that can still be usable for forensic analysis), I can get far enough to see that the PDF within the PDF (i.e. the actual PDF attachment originally sent) was at least partially (de)flate-encoded. Unfortunately, PDFs are binary files with different forms of compression applied; you can\u2019t just use something like strings to extract any usable content. qpdf(1) can be (ab)used to decompress a PDF (while leaving it a PDF) via qpdf --qdf --object-streams=disable input.pdf decompressed.pdf , but, predictably, this doesn\u2019t work when your input is garbled and corrupted: > qpdf --qdf --object-streams=disable recovered.pdf decompressed.pdf WARNING: recovered.pdf: file is damaged\r\nWARNING: recovered.pdf: can't find startxref\r\nWARNING: recovered.pdf: Attempting to reconstruct cross-reference table\r\nWARNING: recovered.pdf (object 34 0, offset 52): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 70): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 85): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 90): unexpected >\r\nWARNING: recovered.pdf (object 34 0, offset 92): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 116): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 121): unknown token while reading object; treating as string\r\nWARNING: recovered.pdf (object 34 0, offset 121): too many errors; giving up on reading object\r\nWARNING: recovered.pdf (object 34 0, offset 125): expected endobj\r\nWARNING: recovered.pdf (object 41 0, offset 9562): expected endstream\r\nWARNING: recovered.pdf (object 41 0, offset 8010): attempting to recover stream length\r\nWARNING: recovered.pdf (object 41 0, offset 8010): unable to recover stream data; treating stream as empty\r\nWARNING: recovered.pdf (object 41 0, offset 9616): expected endobj\r\nWARNING: recovered.pdf (object 41 0, offset 9616): EOF after endobj\r\nqpdf: recovered.pdf: unable to find trailer dictionary while recovering damaged file Between the inconsistent OCR results and the problem with the l vs 1 , it\u2019s not a very encouraging situation. To me, this is a problem begging for a (traditional, non-LLM) ML solution, specifically leveraging the fact that we know the font in question and, roughly, the compression applied. Alas, I don\u2019t have more time to lend to this challenge at the moment, as there are a number of things I set aside just in order to publish this article. So here\u2019s the challenge for anyone I can successfully nerdsnipe: Can you manage to recreate the original PDF from the Content-Transfer-Encoding: base64 output included in the dump? It can\u2019t be that hard, can it? Can you find other attachments included in the latest Epstein dumps that might also be possible to reconstruct? Unfortunately, the contractor that developed the full-text search for the Department of Justice did a pretty crappy job and full-text search is practically broken even accounting for the bad OCR and wrangled quoted-printable decoding (malicious compliance??); nevertheless, searching for Content-Transfer-Encoding and base64 returns a number of results \u2013 it\u2019s just that, unfortunately, most are uselessly truncated or only the SMTP headers from Apple Mail curiously extracted. I have uploaded the original EFTA00400459.pdf from Epstein Dataset 9 as downloaded from the DoJ website to the Internet Archive , as well as the individual pages losslessly encoded to WebP images to save you the time and trouble of converting them yourself. If it\u2019s of any use to anyone, I\u2019ve also uploaded the very-much-invalid Amazon Textract OCR text (from the losslessly 2x\u2019d images), which you can download here . Oh, and one final hint: when trying to figure out 1 vs l , I was able to do this with 100% accuracy only via trial-and-error, decoding one line of base64 text at-a-time, but this only works for the plain-text portions of the PDF (headers, etc). For example, I started with my best guess for one line that I had to type out myself when trying with tesseract, and then was able to (in this case) deduce which particular 1 s or l s were flipped: > pbpaste\r\nSW5mbzw8L01sbHVzdHJhdG9yIDgxIDAgUj4+L1Jlc29lcmNlczw8L0NvbG9yU3BhY2U8PC9DUzAG\r\n> pbpaste | base64 -d\r\nInfo < </ M llustrator 81 0 R> > /Reso e rces < </ColorSpace < </CS0\r\n> \r\n> # which I was able to correct:\r\n>\r\n> pbpaste\r\nSW5mbzw8L0lsbHVzdHJhdG9yIDgxIDAgUj4+L1Jlc291cmNlczw8L0NvbG9yU3BhY2U8PC9DUzAG\r\n> pbpaste | base64 -d\r\nInfo < </Illustrator 81 0 R> > /Resources < </ColorSpace < </CS0 \u2026but good luck getting that to work once you get to the flate-compressed sections of the PDF. I\u2019ll be posting updates on Twitter @mqudsi , and you can reach out to me on Signal at mqudsi.42 if you have anything sensitive you would like to share. You can join in the discussion on Hacker News or on r/netsec . Leave a comment below if you have any ideas/questions, or if you think I missed something! In case you\u2019re wondering, the shell session excerpts in this article are all in fish , which I think is a good fit for string wrangling because of its string builtin with extensive operations with a very human-readable syntax (at least compared to perl or awk , the usual go-tos for string manipulation), and it lets you compose multiple operations as separate commands while not devolving to performing pathologically because no external commands are fork / exec \u2018d because of its builtin nature. And I\u2019m not just saying that because of the blood, sweat, and tears I\u2019ve contributed to the project. \u21a9 I didn\u2019t want to convert the PNGs back to a single PDF as I didn\u2019t want any further loss in quality. \u21a9 Similar Posts Craving more? Here are some posts a vector similarity search turns up as being relevant or similar from our catalog you might also enjoy. CVE-2022-23968: Xerox vulnerability allows unauthenticated users to remotely brick network printers (UPDATED) iMessage for Windows Beware of this new Chrome \"font wasn't found\" hack! 25 thoughts on \u201c Recreating uncensored Epstein PDFs from raw encoded attachments \u201d By playing with the curves using photopea I\u2019m able to improve the readability a bit. I\u2019m applying the curve layer only at the top of the character https://full.ouplo.com/1a/6/ECnz.png And I use a \u201csketch\u201d curve to precisely darken the selection https://full.ouplo.com/1a/6/7msk.png Might also be interesting to extract small jpegs from all the ambiguous \u201c1/l\u201d and then make a website to crowd source the most likely character, like captchas. An alternative approach \u2014 captcha style: \u2013 Divide the image into chunks like 50 characters long \u2013 Put it on a website where anyone can type up the small amount of characters \u2013 Reconstruct the text from the c",
  "comments": [
    {
      "by": "chrisjj",
      "id": 46906794,
      "kids": [
        46908991,
        46907065
      ],
      "parent": 46890335,
      "text": "&gt; it\u2019s safe to say that Pam Bondi\u2019s DoJ did not put its best and brightest on this<p>Or worse. She did.",
      "time": 1770333334,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "dperfect",
      "id": 46907841,
      "kids": [
        46908451,
        46909030,
        46911948
      ],
      "parent": 46890335,
      "text": "Nerdsnipe confirmed :)<p>Claude Opus came up with this script:<p><a href=\"https:&#x2F;&#x2F;pastebin.com&#x2F;ntE50PkZ\" rel=\"nofollow\">https:&#x2F;&#x2F;pastebin.com&#x2F;ntE50PkZ</a><p>It produces a somewhat-readable PDF (first page at least) with this text output:<p><a href=\"https:&#x2F;&#x2F;pastebin.com&#x2F;SADsJZHd\" rel=\"nofollow\">https:&#x2F;&#x2F;pastebin.com&#x2F;SADsJZHd</a><p>(I used the cleaned output at <a href=\"https:&#x2F;&#x2F;pastebin.com&#x2F;UXRAJdKJ\" rel=\"nofollow\">https:&#x2F;&#x2F;pastebin.com&#x2F;UXRAJdKJ</a> mentioned in a comment by Joe on the blog page)",
      "time": 1770341210,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "bawolff",
      "id": 46907147,
      "parent": 46890335,
      "text": "Teseract supports being trained for specific fonts, that would probably be a good starting point<p><a href=\"https:&#x2F;&#x2F;pretius.com&#x2F;blog&#x2F;ocr-tesseract-training-data\" rel=\"nofollow\">https:&#x2F;&#x2F;pretius.com&#x2F;blog&#x2F;ocr-tesseract-training-data</a>",
      "time": 1770335563,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "pyrolistical",
      "id": 46906897,
      "kids": [
        46909776,
        46907087
      ],
      "parent": 46890335,
      "text": "It decodes to binary pdf and there are only so many valid encodings. So this is how I would solve it.<p>1. Get an open source pdf decoder<p>2. Decode bytes up to first ambiguous char<p>3. See if next bits are valid with an 1, if not it\u2019s an l<p>4. Might need to backtrack if both 1 and l were valid<p>By being able to quickly try each char in the middle of the decoding process you cut out the start time. This makes it feasible to test all permutations automatically and linearly",
      "time": 1770333891,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "percentcer",
      "id": 46906904,
      "kids": [
        46907642,
        46906939,
        46906968
      ],
      "parent": 46890335,
      "text": "This is one of those things that seems like a nerd snipe but would be more easily accomplished through brute forcing it. Just get 76 people to manually type out one page each, you&#x27;d be done before the blog post was written.",
      "time": 1770333944,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "legitster",
      "id": 46907459,
      "kids": [
        46910920,
        46907925,
        46910449,
        46907546,
        46907469
      ],
      "parent": 46890335,
      "text": "Given how much of a hot mess PDFs are in general, it seems like it would behoove the government to just develop a new, actually safe format to standardize around for government releases and make it open source.<p>Unlike every other PDF format that has been attempted, the federal government doesn&#x27;t have to worry about adoption.",
      "time": 1770337828,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "ChocMontePy",
      "id": 46907982,
      "kids": [
        46909737
      ],
      "parent": 46890335,
      "text": "You can use the justice.gov search box to find several different copies of that same email.<p>The copy linked in the post:<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA00400459.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA004004...</a><p>Three more copies:<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02153691.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02153...</a><p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02154109.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02154...</a><p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02154246.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA02154...</a><p>Perhaps having several different versions might make it easier.",
      "time": 1770342489,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "pimlottc",
      "id": 46906725,
      "kids": [
        46906905,
        46909263,
        46906785
      ],
      "parent": 46890335,
      "text": "Why not just try every permutation of (1,l)? Let\u2019s see, 76 pages, approx 69 lines per page, say there\u2019s one instance of [1l] per line, that\u2019s only\u2026 uh\u2026 2^5244 possibilities\u2026<p>Hmm. Anyone got some spare CPU time?",
      "time": 1770332938,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "sorbus-25",
      "id": 46909057,
      "kids": [
        46909117
      ],
      "parent": 46890335,
      "text": "Event details: <a href=\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20260206040716&#x2F;https:&#x2F;&#x2F;what2wearwhere.com&#x2F;dubin-breast-center-2nd-annual-benefit&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20260206040716&#x2F;https:&#x2F;&#x2F;what2wear...</a>",
      "time": 1770351435,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "kevin_thibedeau",
      "id": 46907157,
      "parent": 46890335,
      "text": "pdftoppm and Ghostscript (invoked via Imagemagick) re-rasterize full pages to generate their output. That&#x27;s why it was slow. Even worse with a Q16 build of Imagemagick. Better to extract the scanned page images directly with pdfimages or mutool.<p>Followup: pdfimages is 13x faster than pdftoppm",
      "time": 1770335610,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "bushbaba",
      "id": 46908271,
      "kids": [
        46909484,
        46909254,
        46909074
      ],
      "parent": 46890335,
      "text": "This proves my paranoia that you should print and rescan redactions. That or do screenshots of the pdf redacted and convert back to a pdf",
      "time": 1770344953,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "velaia",
      "id": 46907329,
      "parent": 46890335,
      "text": "Bummer that it&#x27;s not December - the <a href=\"https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;adventofcode&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;adventofcode&#x2F;</a> crows would love this puzzle",
      "time": 1770336821,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "nubg",
      "id": 46907602,
      "kids": [
        46908705,
        46910985,
        46907766
      ],
      "parent": 46890335,
      "text": "Wait would this give us the unredacted PDFs?",
      "time": 1770339029,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "ks2048",
      "id": 46909736,
      "parent": 46890335,
      "text": "I wonder if jmail (<a href=\"https:&#x2F;&#x2F;www.jmail.world&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.jmail.world&#x2F;</a>) has worked on this?<p>I tried to find the message in this blog post, but couldn&#x27;t. (don&#x27;t see how to search by date).",
      "time": 1770358971,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "linuxguy2",
      "id": 46906520,
      "parent": 46890335,
      "text": "Love this, absolutely looking forward to some results.",
      "time": 1770331675,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "FarmerPotato",
      "id": 46906642,
      "kids": [
        46906921
      ],
      "parent": 46890335,
      "text": "If only Base64 had used a checksum.",
      "time": 1770332474,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "wtcactus",
      "id": 46910139,
      "kids": [
        46911065
      ],
      "parent": 46890335,
      "text": "My non political take about this gift that keeps on giving is that: PDF might seem great for the end user that is just expected to read or print the file they are given, but the technology actually sucks.<p>PDF is basically a prettify layer on top of the older PS that brings an all lot of baggage. The moment you start trying to do what should be simple stuff like editing lines, merging pages, change resolution of the images, it starts giving you a lot of headaches.<p>I used to have a few scripts around to fight some of its quirks from when I was writing my thesis and had to work daily with it. But well, it was still an improvement over Word.",
      "time": 1770363504,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "iwontberude",
      "id": 46906653,
      "kids": [
        46906870
      ],
      "parent": 46890335,
      "text": "This one is irresistible to play with. Indeed a nerd snipe.",
      "time": 1770332512,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "Evidlo",
      "id": 46908397,
      "kids": [
        46911561
      ],
      "parent": 46890335,
      "text": "I took at stab at training Tesseract and holy jeebus is their CLI awful.  Just an insanely complicated configuration procedure.",
      "time": 1770346051,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "zahlman",
      "id": 46906910,
      "kids": [
        46908346,
        46907060
      ],
      "parent": 46890335,
      "text": "&gt; \u2026but good luck getting that to work once you get to the flate-compressed sections of the PDF.<p>A dynamic programming type approach might still be helpful. One version or other of the character might produce invalid flate data while the other is valid, or might give an implausible result.",
      "time": 1770333967,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "queenkjuul",
      "id": 46908529,
      "parent": 46890335,
      "text": "I&#x27;m only here to shout out fish shell, a shell finally designed for the modern world of the 90s",
      "time": 1770346997,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "eek2121",
      "id": 46907053,
      "kids": [
        46912269
      ],
      "parent": 46890335,
      "text": "Honestly, this is something that should&#x27;ve been kept private, until each and every single one of the files is out in the open. Sure, mistakes are being made, but if you blast them onto the internet, they WILL eventually get fixed.<p>Cool article, however.",
      "time": 1770334879,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "SomaticPirate",
      "id": 46909063,
      "kids": [
        46911081
      ],
      "parent": 46890335,
      "text": "Are there archives of this? I have no doubt after this post goes viral some of these files might go \u201cmissing\u201d\nHaving a large number of conspiracies validated has lead me to firmly plant my aluminum hat",
      "time": 1770351524,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "winddude",
      "id": 46909125,
      "kids": [
        46910539,
        46909275
      ],
      "parent": 46890335,
      "text": "here&#x27;s another few to decode,<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA01804740.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2010&#x2F;EFTA01804...</a><p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA00775520.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA007755...</a><p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA00434905.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA004349...</a><p>and than this one judging by the name of the file (hanna something) and content of the email:<p>&quot;Here is my girl, sweet sparkling Hanna=E2=80=A6!\nI am sure she is on Skype &quot;<p>maybe more sinister (so be careful, i have no ideas what the laws are if you uncover you know what trump and Epstein were into)...<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2011&#x2F;EFTA02715081.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2011&#x2F;EFTA02715...</a><p>[Above is probably a legit modeling CV for HANNA BOUVENG, based on,  <a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA01120466.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA011204...</a>, but still creepy, and doesn&#x27;t seem like there&#x27;s evidence of her being a victim]",
      "time": 1770352100,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "blindriver",
      "id": 46907346,
      "kids": [
        46912256,
        46908250,
        46909547,
        46907569,
        46911611,
        46911129
      ],
      "parent": 46890335,
      "text": "On one hand, the DOJ gets shit because it was taking too long to produce the documents, and then on another, they get shit because there are mistakes in the redacting because there are 3 million pages of documents.",
      "time": 1770336923,
      "type": "comment",
      "depth": 0
    },
    {
      "by": "winddude",
      "id": 46908991,
      "parent": 46906794,
      "text": "there are a few messaging conversations between FB agents early on that are kind of interesting. It would be very interesting to see them about the releases. I sometimes wonder if some was malicious compliance... ie, do a shitty job so the info get&#x27;s out before it get re-redacted... we can hope...",
      "time": 1770350800,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "eek2121",
      "id": 46907065,
      "kids": [
        46907347,
        46907778,
        46907685,
        46907560,
        46907260,
        46911037
      ],
      "parent": 46906794,
      "text": "I mean, the internet is finding all her mistakes for her. She is actually doing alright with this. Crowdsource everything, fix the mistakes. lol.",
      "time": 1770334952,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "pests",
      "id": 46908451,
      "kids": [
        46908541,
        46909093
      ],
      "parent": 46907841,
      "text": "So it was a public event attended by 450 people:<p><a href=\"https:&#x2F;&#x2F;www.mountsinai.org&#x2F;about&#x2F;newsroom&#x2F;2012&#x2F;dubin-breast-center-holds-inaugural-gala\" rel=\"nofollow\">https:&#x2F;&#x2F;www.mountsinai.org&#x2F;about&#x2F;newsroom&#x2F;2012&#x2F;dubin-breast-...</a><p><a href=\"https:&#x2F;&#x2F;www.businessinsider.com&#x2F;dubin-breast-center-benefit-2012-12\" rel=\"nofollow\">https:&#x2F;&#x2F;www.businessinsider.com&#x2F;dubin-breast-center-benefit-...</a><p>Even names match up, but oddly the date is different.",
      "time": 1770346397,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "notpushkin",
      "id": 46909030,
      "kids": [
        46909104
      ],
      "parent": 46907841,
      "text": "&gt; It produces a somewhat-readable PDF (first page at least) with this text output<p>Any chance you could share a screenshot &#x2F; re-export it as a (normalized) PDF? I\u2019m curious about what\u2019s in there, but all of my readers refuse to open it.",
      "time": 1770351139,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "the_real_cher",
      "id": 46911948,
      "parent": 46907841,
      "text": "This is cool!",
      "time": 1770380642,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "pletnes",
      "id": 46909776,
      "parent": 46906897,
      "text": "You might need to backtrack a lot more, due to the intermediate compression step?",
      "time": 1770359383,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "bawolff",
      "id": 46907087,
      "parent": 46906897,
      "text": "Sounds like a job for afl",
      "time": 1770335105,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "jjwiseman",
      "id": 46907642,
      "kids": [
        46908510
      ],
      "parent": 46906904,
      "text": "Or one person types 76 pages. This is a thing people used to do, not all that infrequently. Or maybe you have one friend who will help\u2013cool, you just cut the time in half.",
      "time": 1770339426,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "WolfeReader",
      "id": 46906939,
      "kids": [
        46910017
      ],
      "parent": 46906904,
      "text": "You think compelling 76 people to <i>honestly and accurately</i> transcribe files is something that&#x27;s easy and quick to accomplish.",
      "time": 1770334104,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "fragmede",
      "id": 46906968,
      "kids": [
        46908893,
        46907170
      ],
      "parent": 46906904,
      "text": "&gt; Just get 76 people<p>I consider myself fairly normal in this regard, but I don&#x27;t have 76 friends to ask to do this, so I don&#x27;t know how I&#x27;d go about doing this. Post an ad on craigslist? Fiverr? Seems like a lot to manage.",
      "time": 1770334267,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "gucci-on-fleek",
      "id": 46910920,
      "parent": 46907459,
      "text": "XPS [0] seems to meet these criteria. It supports most of the features of PDF, is an &quot;official&quot; standard, has decent software support (including lots of open source programs), and uses a standard file format (XML). But the tooling is quite a bit worse than it is for PDF, and the file format is still complex enough that redaction would probably be just as hard.<p>DjVu [1] would be another option. It has really good open source tooling available, but it supports substantially less features than PDF, making it not really suitable as a drop-in replacement. The format is relatively simple though, so redaction should be fairly doable.<p>TIFF [2] is already occasionally used for government documents, but it&#x27;s arguably <i>more</i> complex than PDF, so probably not a good choice for this.<p>[0]: <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Open_XML_Paper_Specification\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Open_XML_Paper_Specification</a><p>[1]: <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DjVu\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DjVu</a><p>[2]: <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;TIFF\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;TIFF</a>",
      "time": 1770371514,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "Spooky23",
      "id": 46907925,
      "kids": [
        46909071
      ],
      "parent": 46907459,
      "text": "You\u2019re thinking about this as a nerd.<p>It\u2019s not a tools problem, it\u2019s a problem of malicious compliance and contempt for the law.",
      "time": 1770341958,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "Ekaros",
      "id": 46910449,
      "parent": 46907459,
      "text": "I give any new document format 3 to 5 years until it ends up with similar mess. And that is if it starts well designed and limited.",
      "time": 1770366911,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "derwiki",
      "id": 46907546,
      "kids": [
        46907601,
        46908428
      ],
      "parent": 46907459,
      "text": "JPEG?",
      "time": 1770338462,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "ChocMontePy",
      "id": 46909737,
      "kids": [
        46910038
      ],
      "parent": 46907982,
      "text": "Also, I found a different base64 encoding with a different font here:<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA00775520.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%209&#x2F;EFTA007755...</a><p>This doesn&#x27;t solve the &quot;1 &amp; l&quot; problem for the pdf you are looking at, but it could be useful anyway.",
      "time": 1770358976,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "wahern",
      "id": 46906905,
      "kids": [
        46912872,
        46910964,
        46907935,
        46907186
      ],
      "parent": 46906725,
      "text": "It should be much easier than that. You should should be able to serially test if each edit decodes to a sane PDF structure, reducing the cost similar to how you can crack passwords when the server doesn&#x27;t use a constant-time memcmp. Are PDFs typically compressed by default? If so that makes it even easier given built-in checksums. But it&#x27;s just not something you can do by throwing data at existing tools. You&#x27;ll need to build a testing harness with instrumentation deep in the bowels of the decoders. This kind of work is the polar opposite of what AI code generators or naive scripting can accomplish.",
      "time": 1770333946,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "kalleboo",
      "id": 46909263,
      "parent": 46906725,
      "text": "Easy, just start a crypto currency (Epsteincoin?) based on solving these base64 scans and you&#x27;ll have all the compute you could ever want just lining up",
      "time": 1770353278,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "sorbus-25",
      "id": 46909117,
      "kids": [
        46909278
      ],
      "parent": 46909057,
      "text": "DUBIN BREAST CENTER\nSECOND ANNUAL BENEFIT\nMONDAY, DECEMBER 10, 2012\nHONORING ELISA PORT, MD, FACS\nAND\nTHE RUTTENBERG FAMILY\nHOST\nCYNTHIA MCFADDEN\nSPECIAL MUSICAL PERFORMANCES\nCAROLINE JONES, K&#x27;NAAN,\nHALEY REINHART, THALIA, EMILY WARREN\nMANDARIN ORIENTAL\n7:00PM COCKTAILS\nLOBBY LOUNGE\n8:00PM DINNER AND ENTERTAINMENT\nMANDARIN BALLROOM\nFESTIVE ATTIRE",
      "time": 1770352022,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "phanimahesh",
      "id": 46909484,
      "parent": 46908271,
      "text": "How would that help in this case?",
      "time": 1770356079,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "Snoozus",
      "id": 46909254,
      "parent": 46908271,
      "text": "this would not have helped here",
      "time": 1770353212,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "ryanSrich",
      "id": 46908705,
      "parent": 46907602,
      "text": "That&#x27;s the idea yeah. There are other people actively working on this. You can follow vx-underground on twitter. They&#x27;re tracking it.",
      "time": 1770348393,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "sznio",
      "id": 46910985,
      "parent": 46907602,
      "text": "From the unredacted attachments you could figure out what the redacted content most likely contains. Just like the other sloppy redactions that sometimes hide one party of the conversation, sometimes the other, so you can easily figure out the both sides.",
      "time": 1770371976,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "poyu",
      "id": 46907766,
      "parent": 46907602,
      "text": "I think it&#x27;s the PDF files that were attached to the emails, since they&#x27;re base64 encoded.",
      "time": 1770340512,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "zahlman",
      "id": 46906921,
      "kids": [
        46907303
      ],
      "parent": 46906642,
      "text": "&quot;had used&quot;? Base64 is still in very common use, specifically embedded within JSON and in &quot;data URLs&quot; on the Web.",
      "time": 1770334014,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "direwolf20",
      "id": 46911065,
      "parent": 46910139,
      "text": "It&#x27;s meant as a printer replacement format, hence &quot;print to PDF&quot;. It&#x27;s a computer file format about equivalent to a printed document. Like a printed document, you can&#x27;t just change its structure and recompile it.",
      "time": 1770372715,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "netsharc",
      "id": 46906870,
      "kids": [
        46907059,
        46909913
      ],
      "parent": 46906653,
      "text": "I doubt the PDF would be very interesting. There are enough clues in the human-readable parts: it&#x27;s an invite to a benefit event in New York (filename calls it DBC12) that&#x27;s scheduled on December 10, 2012, 8pm... Good old-fashioned searching could probably uncover what DBC12 was, although maybe not, it probably wasn&#x27;t a public event.<p>The recipient is also named in there...",
      "time": 1770333745,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "subscribed",
      "id": 46911561,
      "parent": 46908397,
      "text": "Gods, I had a flashback just from you mentioning that.<p>I had a reasonably simple problem to solve, slightly weird font and some 10 words in English (I actually only missed one or two blocks for missing letters to cover all I needed).<p>After a couple of days having almost everything (?) I just surrendered. This seems to be intentionally hostile. All the docs scattered across several repositories, no comprehensive examples, etc.<p>Absolutely awful piece of software from this end (training the last gen).",
      "time": 1770376996,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "yunnpp",
      "id": 46908346,
      "parent": 46906910,
      "text": "Time to flex those Leetcode skills.",
      "time": 1770345630,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "misja111",
      "id": 46912269,
      "parent": 46907053,
      "text": "Won&#x27;t that entire DOJ archive already be downloaded for backup by several people?\nIf I&#x27;d be a journalist working on those files, this is the very first thing I would do as soon as those files were published. Just to make sure you have the originals before DOJ can start adding more redactions.",
      "time": 1770382720,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "direwolf20",
      "id": 46911081,
      "parent": 46909063,
      "text": "<a href=\"https:&#x2F;&#x2F;github.com&#x2F;yung-megafone&#x2F;Epstein-Files\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;yung-megafone&#x2F;Epstein-Files</a>",
      "time": 1770372837,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "netsharc",
      "id": 46910539,
      "parent": 46909125,
      "text": "Geezus, with the short CV in your profile, you couldn&#x27;t tell an LLM to decode &quot;filename=utf-8&quot;CV%5F%5F%5FHanna%5FTr%C3%A4ff%5F.pdf&quot;? That&#x27;s not &quot;Bouveng&quot;.<p>Anyway searching for the email sender&#x27;s name, there&#x27;s a screenshot of an email of hers in English offering him a girl as an assistant who is &quot;in top physical shape&quot; (probably not this Hanna girl). That&#x27;s fucking creepy: <a href=\"https:&#x2F;&#x2F;www.expressen.se&#x2F;nyheter&#x2F;varlden&#x2F;epsteins-lofte-till-barbro-ehnbom-for-att-fa-en-kvinnlig-assistent&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.expressen.se&#x2F;nyheter&#x2F;varlden&#x2F;epsteins-lofte-till...</a>",
      "time": 1770367857,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "Snoozus",
      "id": 46909275,
      "kids": [
        46909388
      ],
      "parent": 46909125,
      "text": "this one has a better font, might be a simple copy&amp;paste job",
      "time": 1770353396,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "tclancy",
      "id": 46912256,
      "parent": 46907346,
      "text": "It really doesn\u2019t matter which foot you use to step on your own dick. This could not have been more mishandled if they gave it to an actual snake.",
      "time": 1770382604,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "rapind",
      "id": 46908250,
      "kids": [
        46909465
      ],
      "parent": 46907346,
      "text": "What they are redacting is pretty questionable though. Entire pages being suspiciously redacted with no explanation (which they are supposed to provide). This is just my opinion, but I think it&#x27;s pretty hard to defend them as making an honest and best effort here. Remember they all lied about and changed their story on the Epstein &quot;files&quot; several times now (by all I mean Bondi, Patel, Bongino, and Trump).<p>It&#x27;s really really hard to give them the benefit of the doubt at this point.",
      "time": 1770344839,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "rexpop",
      "id": 46909547,
      "parent": 46907346,
      "text": "&quot;On the one hand the chef gets shit for taking too long, and then on another for undercooked, badly plated dishes.&quot;<p>Incompetence is incompetence.",
      "time": 1770356707,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "thereisnospork",
      "id": 46907569,
      "parent": 46907346,
      "text": "Considering the justice to document ratio that&#x27;s kind of on them regardless.",
      "time": 1770338712,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "subscribed",
      "id": 46911611,
      "parent": 46907346,
      "text": "It&#x27;s pretty clear who they should be reacting (victims&#x2F;minors) and who they shouldn&#x27;t (perpetrators).<p>They wasted months erasing Trump from that instead. So it&#x27;s on them.",
      "time": 1770377414,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "hypeatei",
      "id": 46911129,
      "parent": 46907346,
      "text": "The zeitgeist around the files started with MAGA and their QAnon conspiracy. All the right wing podcasters were pushing a narrative that Trump was secretly working to expose and takedown a global child sex trafficking ring. Well, it turns out, unsurprisingly, that Trump was implicated too and that&#x27;s when they started to do a 180. You can&#x27;t have your cake and eat it too.",
      "time": 1770373235,
      "type": "comment",
      "depth": 1
    },
    {
      "by": "TSiege",
      "id": 46907347,
      "kids": [
        46910362,
        46911736,
        46908201
      ],
      "parent": 46907065,
      "text": "This would be funnier if it wasn\u2019t child porn being unredacted by our government",
      "time": 1770336948,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "helterskelter",
      "id": 46907778,
      "kids": [
        46912805,
        46910138
      ],
      "parent": 46907065,
      "text": "I wonder if this could be intentional. If the datasets are contaminated with CSAM, anybody with a copy is liable to be arrested for possession.<p>More likely it&#x27;s just an oversight, but it could also be CYA for dragging their feet, like &quot;you rushed us, and look at these victims you&#x27;ve retraumatized&quot;. There are software solutions to find nudity and they&#x27;re quite effective.",
      "time": 1770340606,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "dagi3d",
      "id": 46907685,
      "parent": 46907065,
      "text": "the issue is that mistakes can&#x27;t be fixed in the sense once they are discovered, it doesn&#x27;t matter if they are eventually redacted",
      "time": 1770339769,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "rockskon",
      "id": 46907560,
      "parent": 46907065,
      "text": "Yeah - they&#x27;ll take these lessons learned for future batches of releases.",
      "time": 1770338578,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "chrisjj",
      "id": 46907260,
      "kids": [
        46907795
      ],
      "parent": 46907065,
      "text": "Let&#x27;s see her sued for leaking PII. Here in Europe, she&#x27;d be mincemeat.",
      "time": 1770336311,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "rcakebread",
      "id": 46911037,
      "parent": 46907065,
      "text": "Sicko.",
      "time": 1770372455,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "elmomle",
      "id": 46908541,
      "kids": [
        46909400
      ],
      "parent": 46908451,
      "text": "Your links are for the inaugural (first) ball in December 2011; OP&#x27;s text referred to a second annual ball in December 2012.",
      "time": 1770347066,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "nialv7",
      "id": 46909093,
      "kids": [
        46912831,
        46910101,
        46911111
      ],
      "parent": 46908451,
      "text": "looks like we have it. in the end it&#x27;s pretty mundane...",
      "time": 1770351835,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "dperfect",
      "id": 46909104,
      "parent": 46909030,
      "text": "Screenshot: <a href=\"https:&#x2F;&#x2F;imgur.com&#x2F;eWCfYYd\" rel=\"nofollow\">https:&#x2F;&#x2F;imgur.com&#x2F;eWCfYYd</a>",
      "time": 1770351919,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "wildzzz",
      "id": 46908510,
      "kids": [
        46908689
      ],
      "parent": 46907642,
      "text": "Typing 76 pages is easy when it&#x27;s words in a language you understand. WPM is going to be incredibly slow when you actually have to read every character. On top of that, no spaces and no spellcheck so hopefully you didn&#x27;t miss a character.",
      "time": 1770346851,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "altairprime",
      "id": 46910017,
      "parent": 46906939,
      "text": "Non-engineers are perfectly willing to volunteer their time to do drudgery. It&#x27;s one of my opseng career&#x27;s distinguishing specialties: I&#x27;ll do drudgery rather than code when appropriate, rather than avoiding it or sulking about it (as was a common response at work for some number of decades!). Learned that lesson when I was 18 from an internship (where I completely failed to deliver any work product due to trying to code around the work). It&#x27;s part of why I&#x27;m going into accounting: apparently having the stamina for dreary work is <i>rare</i>?!<p>Also look up double&#x2F;triple data-entry systems, where you have multiple people enter the data and then flag and resolve differences. Won&#x27;t protect you from your staff banding together to fuck you over with maliciously bad data, but it&#x27;s incredibly effective to ensure people were Actually Working Their Blocks under healthy circumstances.",
      "time": 1770362057,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "jazzyjackson",
      "id": 46908893,
      "parent": 46906968,
      "text": "First, build a fanbase by streaming on Twitch.",
      "time": 1770350065,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "Krutonium",
      "id": 46907170,
      "kids": [
        46911399
      ],
      "parent": 46906968,
      "text": "Amazon Mechanical Turk?",
      "time": 1770335728,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "legitster",
      "id": 46909071,
      "parent": 46907925,
      "text": "Even the previous justice departments struggled with PDFs. The way they handled it was scrubbing all possible metadata and uploading it as images.<p>For example, when the Mueller reports were released with redactions, they had no searchable text or meta data because they were worried about these exact kind of data leaks.<p>However, vast troves of unsearchable text is not a huge win for transparency.<p>PDFs are just a garbage format and even good administrations struggle.",
      "time": 1770351582,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "legitster",
      "id": 46907601,
      "kids": [
        46911343
      ],
      "parent": 46907546,
      "text": "That&#x27;s not really comparable - It needs to be editable and searchable.",
      "time": 1770339029,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "recursive",
      "id": 46908428,
      "kids": [
        46909888
      ],
      "parent": 46907546,
      "text": "Lossy",
      "time": 1770346232,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "ChocMontePy",
      "id": 46910038,
      "parent": 46909737,
      "text": "And this might be a copy of the original pdf:<p><a href=\"https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2011&#x2F;EFTA02702727.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.justice.gov&#x2F;epstein&#x2F;files&#x2F;DataSet%2011&#x2F;EFTA02702...</a>",
      "time": 1770362287,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "JKCalhoun",
      "id": 46912872,
      "parent": 46906905,
      "text": "Not necessarily a PDF attachment?<p>Someone who made some progress on one Base64 attachment got some XMP metadata that suggested a photo from an iPhone. Now I don&#x27;t know if that photo was itself embedded in a PDF, but perhaps getting at least the first few hundred bytes decoded (even if it had to be done manually) would hint at the file-type of the attachment. Then you could run your tests for file fidelity.",
      "time": 1770386226,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "sznio",
      "id": 46910964,
      "parent": 46906905,
      "text": "&gt;It should be much easier than that. You should should be able to serially test if each edit decodes to a sane PDF structure<p>that&#x27;s pointed out in the article. It&#x27;s easy for plaintext sections, but not for compressed sections. Didn&#x27;t notice any mention of checksums.",
      "time": 1770371815,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "pimlottc",
      "id": 46907935,
      "parent": 46906905,
      "text": "I wonder if you could leverage some of the fuzzing frameworks tools like Jepsen rely on. I\u2019m sure there\u2019s got to be one for PDF generation.",
      "time": 1770342044,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "cluckindan",
      "id": 46907186,
      "kids": [
        46911301
      ],
      "parent": 46906905,
      "text": "On the contrary, that kind of one-off tooling seems a great fit for AI. Just specify the desired inputs, outputs and behavior as accurately as possible.",
      "time": 1770335832,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "sorbus-25",
      "id": 46909278,
      "parent": 46909117,
      "text": "Some pics from the event. Doppelg\u00e4nger in the background?: <a href=\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20121215131412&#x2F;https:&#x2F;&#x2F;thaliadiva.wordpress.com&#x2F;2012&#x2F;12&#x2F;11&#x2F;nuevas-fotos-thalia-en-the-dubin-breast-center-2nd-annual-benefit-10-12-2012&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20121215131412&#x2F;https:&#x2F;&#x2F;thaliadiv...</a>",
      "time": 1770353405,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "bahmboo",
      "id": 46907303,
      "parent": 46906921,
      "text": "&quot;had&quot; in the sense of when it was designed and introduced as a standard",
      "time": 1770336628,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "RajT88",
      "id": 46907059,
      "parent": 46906870,
      "text": "There&#x27;s potentially a lot of files attached and printed out in this fashion.<p>The search on the DOJ website (which we shouldn&#x27;t trust), given the query: &quot;Content-Type: application&#x2F;pdf; name=&quot;, yields maybe a half dozen or so similarly printed BASE64 attachments.<p>There&#x27;s probably lots of images as well attached in the same way (probably mostly junk).  I deleted all my archived copies recently once I learned about how not-quite-redacted they were.  I will leave that exercise to someone else.",
      "time": 1770334904,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "notenlish",
      "id": 46909913,
      "kids": [
        46910435
      ],
      "parent": 46906870,
      "text": "There&#x27;s 70 results that come out when searching for &quot;application&#x2F;pdf&quot; on the doj website",
      "time": 1770360822,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "winddude",
      "id": 46909388,
      "parent": 46909275,
      "text": "I&#x27;ve checked for copy and paste, there&#x27;s so many character flaws, their OCR must have sucked really bad, I may try with deepseekOCR or something. I mean the database would probably more searchable if someone ran every file through a better OCR.",
      "time": 1770354869,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "Rebelgecko",
      "id": 46909465,
      "parent": 46908250,
      "text": "My favorite is that sometimes they redact the word &quot;don&#x27;t&quot;. Not only does it totally change the meaning of whatever sentence it&#x27;s in, the conspiracy theory is that they had a Big Dumb Regex for redacting &#x2F;Don\\W+T&#x2F;i to remove Trump references",
      "time": 1770355699,
      "type": "comment",
      "depth": 2
    },
    {
      "by": "block_dagger",
      "id": 46910362,
      "kids": [
        46911795,
        46910742
      ],
      "parent": 46907347,
      "text": "Weren\u2019t. Subjunctive mood.",
      "time": 1770365869,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "lazide",
      "id": 46911736,
      "parent": 46907347,
      "text": "If you think the child porn is the worst part of this mess, I\u2019ve got news for you.<p>We\u2019d all be lucky if it was <i>just</i> distributing child porn.",
      "time": 1770378558,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "JKCalhoun",
      "id": 46912805,
      "parent": 46907778,
      "text": "I&#x27;ll take Hanlon\u2019s Razor for 500, Alex.",
      "time": 1770385834,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "adaml_623",
      "id": 46910138,
      "kids": [
        46912994
      ],
      "parent": 46907778,
      "text": "Or it&#x27;s distraction. Leave nudity in to use up attention that should be turning to analysis of what&#x27;s been redacted.<p>There&#x27;s redaction to protect victims and there&#x27;s redaction to protect specific co-conspirators in Epstein&#x27;s spy ring",
      "time": 1770363500,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "ISL",
      "id": 46907795,
      "kids": [
        46907921
      ],
      "parent": 46907260,
      "text": "The US administration is, at present, regularly violating the law and ignoring court orders.  Indeed, these very releases are patently in violation of multiple federal laws -- they&#x27;re simultaneously insufficiently-responsive to meet the requirements of the law requiring the release of the files and fall afoul of CSAM laws by being incompletely redacted.<p>The challenge, as we&#x27;re all experiencing together, is that the law is not inherently self-enforcing.",
      "time": 1770340773,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "pests",
      "id": 46909400,
      "kids": [
        46909792
      ],
      "parent": 46908541,
      "text": "You are right my first is incorrect but the second does seem to be from 2012.",
      "time": 1770354962,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "JKCalhoun",
      "id": 46912831,
      "parent": 46909093,
      "text": "There are plenty of other PDF&#x27;s with Base64 encoded attachments.",
      "time": 1770385980,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "klustregrif",
      "id": 46910101,
      "kids": [
        46912981,
        46910560
      ],
      "parent": 46909093,
      "text": "Which begs the question why was it censored?",
      "time": 1770363067,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "ryanSrich",
      "id": 46908689,
      "kids": [
        46912274
      ],
      "parent": 46908510,
      "text": "Seems like a job for an LLM",
      "time": 1770348282,
      "type": "comment",
      "depth": 3
    },
    {
      "by": "subscribed",
      "id": 46911399,
      "parent": 46907170,
      "text": "No, I don&#x27;t think so :) --  <a href=\"https:&#x2F;&#x2F;techcrunch.com&#x2F;2023&#x2F;06&#x2F;14&#x2F;mechanical-turk-workers-are-using-ai-to-automate-being-human&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;techcrunch.com&#x2F;2023&#x2F;06&#x2F;14&#x2F;mechanical-turk-workers-ar...</a>",
      "time": 1770375678,
      "type": "comment",
      "depth": 3
    }
  ]
}